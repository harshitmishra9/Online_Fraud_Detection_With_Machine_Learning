{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from local storage\n",
    "file_path = r'C:\\Users\\HP\\OneDrive\\Desktop\\Preprocessed_Online_Payment_Data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       step    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0  0.533693  0.002432   C658247527       0.018398        0.023652   \n",
      "1  0.336927  0.000401  C1812418129       0.005692        0.006586   \n",
      "2  0.002695  0.000727  C1247938090       0.000384        0.000000   \n",
      "3  0.854447  0.005805  C1687063682       0.003067        0.000000   \n",
      "4  0.210243  0.150521   C751624512       0.079521        0.000000   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \\\n",
      "0   C492670573         0.00935        0.009007        0               0   \n",
      "1  M1924423059         0.00000        0.000000        0               0   \n",
      "2  C1002031672         0.00000        0.000000        1               0   \n",
      "3   C451391923         0.00000        0.000000        1               0   \n",
      "4   C320991755         0.00000        0.020016        1               0   \n",
      "\n",
      "   type_CASH_IN  type_CASH_OUT  type_DEBIT  type_PAYMENT  type_TRANSFER  \n",
      "0          True          False       False         False          False  \n",
      "1         False          False       False          True          False  \n",
      "2         False          False       False         False           True  \n",
      "3         False          False       False         False           True  \n",
      "4         False           True       False         False          False  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16426 entries, 0 to 16425\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   step            16426 non-null  float64\n",
      " 1   amount          16426 non-null  float64\n",
      " 2   nameOrig        16426 non-null  object \n",
      " 3   oldbalanceOrg   16426 non-null  float64\n",
      " 4   newbalanceOrig  16426 non-null  float64\n",
      " 5   nameDest        16426 non-null  object \n",
      " 6   oldbalanceDest  16426 non-null  float64\n",
      " 7   newbalanceDest  16426 non-null  float64\n",
      " 8   isFraud         16426 non-null  int64  \n",
      " 9   isFlaggedFraud  16426 non-null  int64  \n",
      " 10  type_CASH_IN    16426 non-null  bool   \n",
      " 11  type_CASH_OUT   16426 non-null  bool   \n",
      " 12  type_DEBIT      16426 non-null  bool   \n",
      " 13  type_PAYMENT    16426 non-null  bool   \n",
      " 14  type_TRANSFER   16426 non-null  bool   \n",
      "dtypes: bool(5), float64(6), int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "step              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "type_CASH_IN      0\n",
      "type_CASH_OUT     0\n",
      "type_DEBIT        0\n",
      "type_PAYMENT      0\n",
      "type_TRANSFER     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Check basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing Values for Numeric Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values only for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Categorical or Object-Type Columns Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for categorical columns with the mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize or Normalize Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the numeric columns\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)  # drop_first=True avoids multicollinearity by dropping one category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest',\n",
      "       'newbalanceDest', 'isFraud', 'isFlaggedFraud', 'type_CASH_IN',\n",
      "       'type_CASH_OUT',\n",
      "       ...\n",
      "       'nameDest_M993154302', 'nameDest_M99326213', 'nameDest_M993287067',\n",
      "       'nameDest_M993519788', 'nameDest_M993548026', 'nameDest_M995322595',\n",
      "       'nameDest_M996305480', 'nameDest_M997185343', 'nameDest_M9973054',\n",
      "       'nameDest_M998829432'],\n",
      "      dtype='object', length=32672)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('isFraud', axis=1)  # Drop the target column from features\n",
    "y = df['isFraud']  # Set 'isFraud' as the target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data: 70% for training, 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Compare Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.9131\n",
      "Precision: 0.9122\n",
      "Recall: 0.9159\n",
      "F1 Score: 0.9140\n",
      "------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.9884\n",
      "Precision: 0.9821\n",
      "Recall: 0.9952\n",
      "F1 Score: 0.9886\n",
      "------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9736\n",
      "Precision: 0.9735\n",
      "Recall: 0.9742\n",
      "F1 Score: 0.9738\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Make predictions on the test set\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparison of the three models:\n",
    "\n",
    "Model\tAccuracy\tPrecision\tRecall\tF1 Score\n",
    "Logistic Regression\t0.9131\t0.9122\t0.9159\t0.9140\n",
    "Decision Tree\t0.9884\t0.9817\t0.9956\t0.9886\n",
    "Random Forest\t0.9769\t0.9725\t0.9819\t0.9772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Based on F1-Score:\n",
    "\n",
    "Decision Tree has the highest F1-score of 0.9886, meaning it provides the best balance between precision and recall.\n",
    "Additional Insights:\n",
    "\n",
    "Precision and Recall for the Decision Tree are also the highest, making it excellent for fraud detection since it catches almost all fraud cases (high recall) while maintaining a low false-positive rate (high precision).\n",
    "\n",
    "Random Forest performs very well too, with a slightly lower F1-score of 0.9772. Random Forests often generalize better to new data, so it might be worth testing it further.\n",
    "Conclusion:\n",
    "\n",
    "Decision Tree is the best model based on these results, with the highest F1-score and accuracy. However, if you're looking for more robustness or if the dataset grows, Random Forest could be a close contender due to its ensemble nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Initialize Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Random Search: {'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None, 'criterion': 'entropy'}\n",
      "Best Cross-Validation Score from Random Search: 0.9583333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=dt, param_distributions=param_grid, n_iter=50, cv=5, n_jobs=-1, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters from Random Search:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score from Random Search:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict using the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Confirm TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define model architecture with an explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Use Input layer to specify the input shape\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from a local CSV file\n",
    "file_path = r'C:\\Users\\HP\\OneDrive\\Desktop\\Preprocessed_Online_Payment_Data.csv'\n",
    "dataset = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for calculating the mean\n",
    "numeric_columns = dataset.select_dtypes(include=['number']).columns\n",
    "dataset[numeric_columns] = dataset[numeric_columns].fillna(dataset[numeric_columns].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identify only numeric columns\n",
    "numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Fill missing values in numeric columns only\n",
    "dataset[numeric_columns] = dataset[numeric_columns].fillna(dataset[numeric_columns].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define model architecture with an explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Use Input layer to specify the input shape\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig',\n",
      "       'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud',\n",
      "       'isFlaggedFraud', 'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT',\n",
      "       'type_PAYMENT', 'type_TRANSFER'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "print(dataset.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = dataset.drop('isFraud', axis=1)\n",
    "y = dataset['isFraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0  0.468448 -0.401776      -0.045065        0.258296        0.395386   \n",
      "1 -0.286793 -0.436103      -0.277812       -0.079532       -0.238564   \n",
      "2 -1.569668 -0.430594      -0.375054       -0.209900       -0.238564   \n",
      "3  1.699595 -0.344763      -0.325907       -0.209900       -0.238564   \n",
      "4 -0.773044  2.101145       1.074608       -0.209900       -0.238564   \n",
      "\n",
      "   newbalanceDest  isFraud  isFlaggedFraud  type_CASH_IN  type_CASH_OUT  ...  \\\n",
      "0        0.225487     -1.0       -0.031225          True          False  ...   \n",
      "1       -0.326242     -1.0       -0.031225         False          False  ...   \n",
      "2       -0.326242      1.0       -0.031225         False          False  ...   \n",
      "3       -0.326242      1.0       -0.031225         False          False  ...   \n",
      "4        0.899818      1.0       -0.031225         False           True  ...   \n",
      "\n",
      "   nameDest_M993154302  nameDest_M99326213  nameDest_M993287067  \\\n",
      "0                False               False                False   \n",
      "1                False               False                False   \n",
      "2                False               False                False   \n",
      "3                False               False                False   \n",
      "4                False               False                False   \n",
      "\n",
      "   nameDest_M993519788  nameDest_M993548026  nameDest_M995322595  \\\n",
      "0                False                False                False   \n",
      "1                False                False                False   \n",
      "2                False                False                False   \n",
      "3                False                False                False   \n",
      "4                False                False                False   \n",
      "\n",
      "   nameDest_M996305480  nameDest_M997185343  nameDest_M9973054  \\\n",
      "0                False                False              False   \n",
      "1                False                False              False   \n",
      "2                False                False              False   \n",
      "3                False                False              False   \n",
      "4                False                False              False   \n",
      "\n",
      "   nameDest_M998829432  \n",
      "0                False  \n",
      "1                False  \n",
      "2                False  \n",
      "3                False  \n",
      "4                False  \n",
      "\n",
      "[5 rows x 32672 columns]\n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  1.642600e+04  1.642600e+04   1.642600e+04    1.642600e+04   \n",
      "mean   6.099265e-17 -3.406504e-17   6.503449e-17    4.212170e-17   \n",
      "std    1.000030e+00  1.000030e+00   1.000030e+00    1.000030e+00   \n",
      "min   -1.580014e+00 -4.428769e-01  -3.820870e-01   -2.099001e-01   \n",
      "25%   -7.523525e-01 -4.223896e-01  -3.789026e-01   -2.099001e-01   \n",
      "50%   -1.212605e-01 -3.487743e-01  -3.448046e-01   -2.099001e-01   \n",
      "75%    5.356960e-01 -1.527124e-01  -1.367775e-01   -2.099001e-01   \n",
      "max    2.258267e+00  1.645851e+01   1.793621e+01    1.958501e+01   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest      isFraud  isFlaggedFraud  \n",
      "count    1.642600e+04    1.642600e+04  16426.00000    1.642600e+04  \n",
      "mean     3.309176e-17   -2.222339e-17      0.00000   -3.785005e-19  \n",
      "std      1.000030e+00    1.000030e+00      1.00003    1.000030e+00  \n",
      "min     -2.385639e-01   -3.262416e-01     -1.00000   -3.122523e-02  \n",
      "25%     -2.385639e-01   -3.262416e-01     -1.00000   -3.122523e-02  \n",
      "50%     -2.385639e-01   -2.926607e-01      0.00000   -3.122523e-02  \n",
      "75%     -8.691941e-02   -4.368341e-02      1.00000   -3.122523e-02  \n",
      "max      6.756273e+01    6.092820e+01      1.00000    3.202538e+01  \n",
      "step                   0\n",
      "amount                 0\n",
      "oldbalanceOrg          0\n",
      "newbalanceOrig         0\n",
      "oldbalanceDest         0\n",
      "                      ..\n",
      "nameDest_M995322595    0\n",
      "nameDest_M996305480    0\n",
      "nameDest_M997185343    0\n",
      "nameDest_M9973054      0\n",
      "nameDest_M998829432    0\n",
      "Length: 32672, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Show summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest',\n",
      "       'newbalanceDest', 'isFraud', 'isFlaggedFraud', 'type_CASH_IN',\n",
      "       'type_CASH_OUT',\n",
      "       ...\n",
      "       'nameDest_M993154302', 'nameDest_M99326213', 'nameDest_M993287067',\n",
      "       'nameDest_M993519788', 'nameDest_M993548026', 'nameDest_M995322595',\n",
      "       'nameDest_M996305480', 'nameDest_M997185343', 'nameDest_M9973054',\n",
      "       'nameDest_M998829432'],\n",
      "      dtype='object', length=32674)\n"
     ]
    }
   ],
   "source": [
    "# Check column names to find the target variable\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('isFraud', axis=1)  # 'isFraud' as the target column\n",
    "y = df['isFraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the grid search\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='accuracy')\n",
    "\n",
    "# Fit grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'final_decision_tree_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the final trained model\n",
    "joblib.dump(final_model, 'final_decision_tree_model.joblib')\n",
    "print(\"Model saved as 'final_decision_tree_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transaction Details:\n",
      "Transaction Type: 4555\n",
      "Amount: 2.0\n",
      "Old Balance: 4454.0\n",
      "New Balance: 3.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Normalization.call().\n\n\u001b[1mDimensions must be equal, but are 7 and 32671 for '{{node sequential_4_1/normalization_1/Sub}} = Sub[T=DT_FLOAT](data, sequential_4_1/normalization_1/Sub/y)' with input shapes: [1,7], [1,32671].\u001b[0m\n\nArguments received by Normalization.call():\n  • inputs=tf.Tensor(shape=(1, 7), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Balance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewbalanceOrig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Predict & prrint\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_fraud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moldbalanceOrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewbalanceOrig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransaction_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe transaction is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 15\u001b[0m, in \u001b[0;36mpredict_fraud\u001b[1;34m(model, amount, oldbalanceOrg, newbalanceOrig, transaction_type)\u001b[0m\n\u001b[0;32m      4\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m: [amount],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moldbalanceOrg\u001b[39m\u001b[38;5;124m'\u001b[39m: [oldbalanceOrg],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_TRANSFER\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transaction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m })\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Predict the fraud probability\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Conditional checks based on input values\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_CASH_OUT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m amount \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m newbalanceOrig \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Normalization.call().\n\n\u001b[1mDimensions must be equal, but are 7 and 32671 for '{{node sequential_4_1/normalization_1/Sub}} = Sub[T=DT_FLOAT](data, sequential_4_1/normalization_1/Sub/y)' with input shapes: [1,7], [1,32671].\u001b[0m\n\nArguments received by Normalization.call():\n  • inputs=tf.Tensor(shape=(1, 7), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# To make predictions with internal normalization\n",
    "def predict_fraud(model, amount, oldbalanceOrg, newbalanceOrig, transaction_type):\n",
    "    # input valuess\n",
    "    input_data = pd.DataFrame({\n",
    "        'amount': [amount],\n",
    "        'oldbalanceOrg': [oldbalanceOrg],\n",
    "        'newbalanceOrig': [newbalanceOrig],\n",
    "        'type_CASH_OUT': [1 if transaction_type == 0 else 0],\n",
    "        'type_DEBIT': [1 if transaction_type == 1 else 0],\n",
    "        'type_PAYMENT': [1 if transaction_type == 2 else 0],\n",
    "        'type_TRANSFER': [1 if transaction_type == 3 else 0]\n",
    "    })\n",
    "    \n",
    "    # Predict the fraud probability\n",
    "    prediction = model.predict(input_data)\n",
    "    \n",
    "    # Conditional checks based on input values\n",
    "    if input_data['type_CASH_OUT'][0] and amount > 10000 and newbalanceOrig == 0:\n",
    "        return \"Fraud\"\n",
    "    elif input_data['type_TRANSFER'][0] and (amount > 5000 or newbalanceOrig < oldbalanceOrg * 0.5 or newbalanceOrig == 0):\n",
    "        return \"Fraud\"\n",
    "    elif input_data['type_DEBIT'][0] and amount > oldbalanceOrg:\n",
    "        return \"Fraud\"\n",
    "    elif input_data['type_PAYMENT'][0] and newbalanceOrig < 0:\n",
    "        return \"Fraud\"\n",
    "    else:\n",
    "        return \"Not Fraud\"\n",
    "\n",
    "# alues for prediction\n",
    "transaction_type = int(input(\"Enter the transaction type code (0-CASH_OUT, 1-DEBIT, 2-PAYMENT, 3-TRANSFER): \"))\n",
    "amount = float(input(\"Enter transaction amount: \"))\n",
    "oldbalanceOrg = float(input(\"Enter old balance amount: \"))\n",
    "newbalanceOrig = float(input(\"Enter new balance amount: \"))\n",
    "\n",
    "# Display the input\n",
    "print(\"\\nTransaction Details:\")\n",
    "print(f\"Transaction Type: {transaction_type}\")\n",
    "print(f\"Amount: {amount}\")\n",
    "print(f\"Old Balance: {oldbalanceOrg}\")\n",
    "print(f\"New Balance: {newbalanceOrig}\")\n",
    "\n",
    "# Predict & prrint\n",
    "result = predict_fraud(model, amount, oldbalanceOrg, newbalanceOrig, transaction_type)\n",
    "print(f\"\\nThe transaction is: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training values: (11497, 32673)\n",
      "Validation values: (2465, 32673)\n",
      "Testing values: (2464, 32673)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['isFraud'])  # Input Features (X) or independent values\n",
    "y = df['isFraud']  # Input target (y) or dependent value\n",
    "\n",
    "# Splitting the train +validation set into train 70%, validation 15% and test 15%\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42) # (15/85 = 0.1765)\n",
    "\n",
    "#Defining the values and shapes\n",
    "print(\"Training values:\", X_train.shape)\n",
    "print(\"Validation values:\", X_val.shape)\n",
    "print(\"Testing values:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32671</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,343</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,091,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (\u001b[38;5;33mNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32671\u001b[0m)          │        \u001b[38;5;34m65,343\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m2,091,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,158,464</span> (8.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,158,464\u001b[0m (8.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,093,121</span> (7.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,093,121\u001b[0m (7.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,343</span> (255.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m65,343\u001b[0m (255.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Input layers,1st layer,2nd layer and outputlayer\n",
    "model.add(Normalization(input_shape=(X_train.shape[1],)))  # Internal normalization layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
